{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bayesian Network to Analyse The Risks From Security Breach\n",
    "\n",
    "When there is a securitiy breach, companies are required by regulations to evaluate the severity and the impact of that breach.\n",
    "\n",
    "## Existing Risk Rating Methodologies\n",
    "\n",
    "There are few risk rating methodology out there: ISO, COSO, Mehari, ISF, etc. My favorite is the [OWASP Risk Rating methodology](https://www.owasp.org/index.php/OWASP_Risk_Rating_Methodology) not because of it's coverage or precision, but because of it's speed. In situation where there are few hundred vulnerabilities, simplicity is necessary to provide a risk evaluation in a reasonable time.\n",
    "\n",
    "```\n",
    "Risk = Likelihood * Impact\n",
    "```\n",
    "\n",
    "To get a risk, we just need to evaluate the likelihood that a vulnerability will be exploited, and multiply this by the impact.\n",
    "\n",
    "A grid that looks like this is normally used afterward to figure out what the risk number means: \n",
    "Source: [OWASP Risk Rating methodology](https://www.owasp.org/index.php/OWASP_Risk_Rating_Methodology)\n",
    "\n",
    "<table cellspacing=\"0\" cellpadding=\"5\" border=\"1\" align=\"center\">\n",
    "<tbody><tr>\n",
    "<th colspan=\"5\" align=\"center\">Overall Risk Severity</th>\n",
    "</tr>\n",
    "<tr>\n",
    "<th rowspan=\"4\" width=\"15%\" align=\"center\">Impact</th>\n",
    "<td width=\"15%\" align=\"center\">HIGH</td>\n",
    "<td width=\"15%\" bgcolor=\"orange\" align=\"center\">Medium</td>\n",
    "<td width=\"15%\" bgcolor=\"red\" align=\"center\">High</td>\n",
    "<td width=\"15%\" bgcolor=\"pink\" align=\"center\">Critical</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td align=\"center\">MEDIUM</td>\n",
    "<td bgcolor=\"yellow\" align=\"center\">Low</td>\n",
    "<td bgcolor=\"orange\" align=\"center\">Medium</td>\n",
    "<td bgcolor=\"red\" align=\"center\">High</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td align=\"center\">LOW</td>\n",
    "<td bgcolor=\"lightgreen\" align=\"center\">Note</td>\n",
    "<td bgcolor=\"yellow\" align=\"center\">Low</td>\n",
    "<td bgcolor=\"orange\" align=\"center\">Medium</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td align=\"center\">&nbsp;</td>\n",
    "<td align=\"center\">LOW</td>\n",
    "<td align=\"center\">MEDIUM</td>\n",
    "<td align=\"center\">HIGH</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td align=\"center\">&nbsp;</td>\n",
    "<th colspan=\"4\" align=\"center\">Likelihood</th>\n",
    "</tr>\n",
    "</tbody></table>\n",
    "\n",
    "But how about the vulnerabilities that did materialize? After performing an assessment, the analyst would have gained evidences that would have an influence on the overall risk rating, but by how much?\n",
    "\n",
    "## Updating Our Risk Rating From Evidences\n",
    "\n",
    "In an attempt to answer to this question, my hypothesis is that we can use a Bayesian Network, start from our initial risk rating prior, and then, by adding factors, we can update our risk probability.\n",
    "\n",
    "Why is this important? The General Data Protection Regulation (GDPR) in Europe requires companies to report security risks if they are judged likely to impact data subject, but there isn't a formula provided to do so.\n",
    "\n",
    "### Guidances from the ICO and EU\n",
    "The UK Information Commisionner Office and the European Union provides the following guidance:\n",
    "* From the ICO: [Guide to the General Data Protection Regulation - Personal Data Breaches](https://ico.org.uk/for-organisations/guide-to-the-general-data-protection-regulation-gdpr/personal-data-breaches/)\n",
    "* From the Europan Union: [Guidelines On Personal data breach notification under Regulation 2016/679 wp250rev.01]( http://ec.europa.eu/newsroom/article29/item-detail.cfm?item_id=612052). Relevant Part: section IV of the Article 29\n",
    "\n",
    "## Scenario\n",
    "In a case like the Equifax breach, there is no question: The final risk can be evaluated as **The end of the world**... well, for anybody with a conscience.\n",
    "\n",
    "For other type of incidents, it's not always clean cut. For example, the incident at [Twitter where clear text passwords were found in their logs](https://blog.twitter.com/official/en_us/topics/company/2018/keeping-your-account-secure.html). Personal data (sensitive) was exposed... but was it leaked? While the passwords were in the logs, did anyone abused them? Do we need to report this breach to the authorities? How does our public image will suffer from this incident if we report it publicly?\n",
    "\n",
    "Spoiler: Twitter fixed the issue, resetted the passwords of all affected users (pretty much all), then issued a public statement explaining their conclusions and action taken. The public image didn't suffer at all. They actually became a role model on how to manage to an incident.\n",
    "\n",
    "As an exercise, let's use the twitter password incident as a background context. From there, we can try make up some criterias that they might have evaluated. With all this configured, we can then create scenarios and query our model.\n",
    "\n",
    "## Evaluation Criterias\n",
    "The Europan Union Guidelines on Personal data breach notification mentions that the following factors should be evaluated.\n",
    "* Type of breach: Confidentiality breach\n",
    "* Nature of the personal data: Passwords\n",
    "* Sensitivity of the personal data: sensitive\n",
    "* Volume of personal data: All users\n",
    "* Ease of identification of individuals: Easy\n",
    "* Special characteristics of the individuals: No medical data or anything that sensitive, but Twitter users could have sensitive private direct messages\n",
    "* Special characteristics of the data controller: messages, names, phone numbers\n",
    "* Number of affected individuals: Millions\n",
    "* General points to identify: \n",
    "    * Severity\n",
    "    * Potential impact\n",
    "    * Likelihood of these occurring\n",
    "\n",
    "## Exploitation scenario\n",
    "\n",
    "For our threat actor: let's say that the most likely one would be an employee or partner staff. That employee would have access to the user logs.\n",
    "\n",
    "Twitter has: \n",
    "* ~3,400 employees\n",
    "* ~335,000,000 Monthly active users\n",
    "* 10TB of logs per day (guestimate, I have no source for that number)\n",
    "\n",
    "\n",
    "Question 1: What is the likelihood that an employee would notice the passwords and use them for their personal interest?\n",
    "Hypothesis 1: Given how many employees Twitter has. Let's be pesimistic:\n",
    "* Percentage of employees who have access to the user logs: 55% (engineering company, most likely that all engineers have access to logs for debugging purposes), so ~1870 employees should have access to the logs\n",
    "* Guestimate of the likelihood that somebody would notice the passwords: 2%, about 37.4 employees would notice the passwords\n",
    "* Guestimate of the likelihood that someone who noticed the data would try to abuse it: 1%, 0.37 employee might want to use the password for his personal gain\n",
    "\n",
    "Question 2: What is the potential gain?\n",
    "Hypothesis 2: Potential gain could expect from using that data: potentially high, because we know that users reuse their passwords everywhere.\n",
    "\n",
    "Question 3: What are the likely outcome if an employee is caught exploiting these passwords?\n",
    "* Consequences for the employee if caught: really, really bad: loss of job, criminal record"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building the Our Model\n",
    "\n",
    "This notebook builds over my previous [Student Network Bayes Network notebook](https://github.com/cerebraljam/simple_bayes_network_notebook). However, for this one, the complexity is much higher.\n",
    "\n",
    "First, we build the structure, which consists of the relation between each variable.\n",
    "\n",
    "Then we configure the variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "structures = [('S', 'V'), ('F', 'V'), ('V', 'R'), \n",
    "              ('Q', 'I'), ('I', 'R'), ('E', 'N'), ('N', 'X'), \n",
    "              ('A', 'X'), ('P', 'X'), ('W', 'D'), ('X', 'D'), \n",
    "              ('B', 'D'), ('K', 'C'), ('Y', 'C'), \n",
    "              ('C', 'D'), ('D', 'I')]\n",
    "\n",
    "variables = {}\n",
    "\n",
    "variables['S'] = {\n",
    "    'desc': \"Sensitivity\",\n",
    "    'legend': {0: 'Low', 1: 'High'},\n",
    "    'cpd': { 0: 0.1, 1: 0.9} # Passwords are unlikely to not be sensitive (10%), and likely to be sensitive (90%)\n",
    "}\n",
    "\n",
    "variables['F'] = {\n",
    "    'desc': \"Fraud Detection\",\n",
    "    'legend': {0: 'unnoticed', 1: 'Noticed'},\n",
    "    'cpd': { 0: 0.2, 1: 0.8} # Likely hood that an increase in password abuse would be noticed\n",
    "}\n",
    "\n",
    "variables['V'] = {\n",
    "    'desc': \"Severity\",\n",
    "    'legend': { 0:'Low', 1:'High' },\n",
    "    'cpd': {\n",
    "        0: { 'S': { 0: { 'F': { 0: 0.8, 1: 0.4 } }, # Severity is Low if Severity = Low, Fraud = No Increase (80%) or Noticed (40%)\n",
    "                    1: { 'F': { 0: 0.7, 1: 0.2 } } } }, # Severity is Low if Severity = Low, Fraud = No Increase (70%) or Noticed (20%)\n",
    "        1: { 'S': { 0: { 'F': { 0: 0.2, 1: 0.6 } }, # Severity is High if Severity = Low, Fraud = No Increase (20%) or Noticed (60%)\n",
    "                    1: { 'F': { 0: 0.3, 1: 0.8 } } } }, # Severity is High if Severity = High, Fraud = No Increase (30%) or Noticed (80%)\n",
    "    }\n",
    "}\n",
    "\n",
    "variables['Q'] = {\n",
    "    'desc': \"Quantity\",\n",
    "    'legend': {0: 'Little', 1: 'A lot'},\n",
    "    'cpd': { 0: 0.3, 1: 0.7} # Small quantity of account leaked (30%), Big quantity of account leaked (70%)\n",
    "}\n",
    "    \n",
    "variables['E'] = {\n",
    "    'desc': \"Employees\",\n",
    "    'legend': {0: 'No access', 1: 'Access'},\n",
    "    'cpd': { 0: 0.45, 1: 0.55} # % Employees who don't have / have access to the logs\n",
    "}\n",
    "\n",
    "variables['N'] = {\n",
    "    'desc': \"Notice\",\n",
    "    'legend': { 0:'Not looking', 1:'Find' },\n",
    "    'cpd': {\n",
    "        0: { 'E': { 0: 0.999, 1: 0.01} }, # Probability that an employee don't notice if they have no access, don't notice if they have access\n",
    "        1: { 'E': { 0: 0.001, 1: 0.99} } # Probability of finding if the employee don't have access / if the employee have access\n",
    "    }\n",
    "}\n",
    "\n",
    "variables['A'] = {\n",
    "    'desc': \"Ease\",\n",
    "    'legend': {0: 'Hard', 1: 'Easy'},\n",
    "    'cpd': { 0: 0.3, 1: 0.7} # Correlating the password found with other user information, necessary to exploit the passwords\n",
    "}\n",
    "\n",
    "variables['P'] = {\n",
    "    'desc': \"Speed\",\n",
    "    'legend': {0: 'Slow', 1: 'Fast'},\n",
    "    'cpd': { 0: 0.8, 1: 0.2} # How fast does the extraction of an exploitable user profile (user/pass) takes?\n",
    "}\n",
    "\n",
    "variables['B'] = {\n",
    "    'desc': \"Noise\",\n",
    "    'legend': {0: 'Noisy', 1: 'Unnoticed'},\n",
    "    'cpd': { 0: 0.7, 1: 0.3} # How likely that an extraction of the user profile would be noticed by the monitoring services.\n",
    "}\n",
    "\n",
    "variables['X'] = {\n",
    "    'desc': \"Exploitable\",\n",
    "    'legend': { 0:'Low', 1:'High' },\n",
    "    'cpd': {\n",
    "        0: { 'N': { 0: { 'A': { 0: { 'P': {0: 0.99, 1: 0.8} },\n",
    "                                1: { 'P': {0: 0.7, 1: 0.7} } } },\n",
    "                    1: { 'A': { 0: { 'P': {0: 0.5, 1: 0.7} },\n",
    "                                1: { 'P': {0: 0.7, 1: 0.2} } } } } },\n",
    "        1: { 'N': { 0: { 'A': { 0: { 'P': {0: 0.01, 1: 0.2} },\n",
    "                                1: { 'P': {0: 0.3, 1: 0.3} } } },\n",
    "                    1: { 'A': { 0: { 'P': {0: 0.5, 1: 0.3} },\n",
    "                                1: { 'P': {0: 0.3, 1: 0.8} } } } } }\n",
    "    }\n",
    "}\n",
    "    \n",
    "variables['K'] = {\n",
    "    'desc': \"Job\",\n",
    "    'legend': {0: 'Keep', 1: 'Fired'}, \n",
    "    'cpd': { 0: 0.1, 1: 0.9} # How likely that the employee will keep his job if (s)he is discovered extracting the user profiles?\n",
    "}\n",
    "    \n",
    "variables['Y'] = {    \n",
    "    'desc': \"Criminal Record\",\n",
    "    'legend': {0: 'Ignored', 1: 'Get'},\n",
    "    'cpd': { 0: 0.05, 1: 0.95} # How likely that the employee will get a criminal record if (s)he is discovered extracting the user profiles?\n",
    "}\n",
    "    \n",
    "variables['W'] = {    \n",
    "    'desc': \"Reward\",\n",
    "    'legend': {0: 'Low', 1: 'High'},\n",
    "    'cpd': { 0: 0.6, 1: 0.4} # What is the likelihood of getting a high reward by extrating the user profiles?\n",
    "}\n",
    "    \n",
    "variables['C'] = {    \n",
    "    'desc': \"Consequences\",\n",
    "    'legend': { 0:'little', 1:'severe' },\n",
    "    'cpd': {\n",
    "        0: { 'K': { 0: { 'Y': { 0: 0.9, 1: 0.2 } }, # Likelihood to have little consequences if the employee keeps his job and (don't) get a criminal record\n",
    "                    1: { 'Y': { 0: 0.4, 1: 0.1 } } } }, # Likelihood to have little consequences if the employee is fired and (don't) get a criminal record\n",
    "        1: { 'K': { 0: { 'Y': { 0: 0.1, 1: 0.8 } }, # Likelihood to have severe consequences if the employee is not fired and (don't) get a criminal record\n",
    "                    1: { 'Y': { 0: 0.6, 1: 0.9 } } } }, # Likelihood to have severe consequences if the employee is fired and (don't) get a criminal record\n",
    "    }\n",
    "}\n",
    "    \n",
    "variables['D'] = {\n",
    "    'desc': \"Is Exploited\",\n",
    "    'legend': { 0:'Will exploit', 1:'Will not' }, # How likely it is that an employee will exploit given...\n",
    "    'cpd': {\n",
    "        0: { 'W': { 0: { 'X': { 0: { 'B': { 0: { 'C': {0: 0.99, 1: 0.0001} }, # will do it if: low reward, low exploitability, will be detected, little/severe consequence\n",
    "                                            1: { 'C': {0: 0.9, 1: 0.0005} } } }, # will do it if: low reward, low exploitability, will be detected, little/severe consequence\n",
    "                                1: { 'B': { 0: { 'C': {0: 0.98, 1: 0.005} }, # will do it if: low reward, exploitable, will not be detected, little/severe consequence\n",
    "                                            1: { 'C': {0: 0.85, 1: 0.01} } } } } }, # will do it if: low reward, low exploitability, will be detected, little/severe consequence\n",
    "                    1: { 'X': { 0: { 'B': { 0: { 'C': {0: 0.8, 1: 0.005} }, # will do it if: high reward, low exploitability, will not be detected, little/severe consequence\n",
    "                                            1: { 'C': {0: 0.7, 1: 0.008} } } }, # will do it if: high reward, low exploitability, will be detected, little/severe consequence\n",
    "                                1: { 'B': { 0: { 'C': {0: 0.4, 1: 0.02} }, # will do it if: high reward, exploitable, will be detected, little/severe consequence\n",
    "                                            1: { 'C': {0: 0.2, 1: 0.01} } } } } } } }, # will do it if: high reward, exploitable, will not be detected, little/severe consequence\n",
    "        1: { 'W': { 0: { 'X': { 0: { 'B': { 0: { 'C': {0: 0.01, 1: 0.9999} }, # will not do it if: low reward, low exploitability, will be detected, little/severe consequence\n",
    "                                            1: { 'C': {0: 0.1, 1: 0.9995} } } }, # will not do it if: low reward, low exploitability, will be detected, little/severe consequence\n",
    "                                1: { 'B': { 0: { 'C': {0: 0.02, 1: 0.995} }, # will not do it if: low reward, exploitable, will not be detected, little/severe consequence\n",
    "                                            1: { 'C': {0: 0.15, 1: 0.99} } } } } }, # will not do it if: if: low reward, low exploitability, will be detected, little/severe consequence\n",
    "                    1: { 'X': { 0: { 'B': { 0: { 'C': {0: 0.2, 1: 0.995} }, # will not do it if: high reward, low exploitability, will not be detected, little/severe consequence\n",
    "                                            1: { 'C': {0: 0.3, 1: 0.992} } } }, # will not do it if: high reward, low exploitability, will not be detected, little/severe consequence\n",
    "                                1: { 'B': { 0: { 'C': {0: 0.6, 1: 0.98} }, # will not do it if: high reward, exploitable, will be detected, little/severe consequence\n",
    "                                            1: { 'C': {0: 0.8, 1: 0.99} } } } } } } } # will not do it if: high reward, exploitable, will not be detected, little/severe consequence\n",
    "    }\n",
    "}\n",
    "    \n",
    "variables['I'] = {    \n",
    "    'desc': \"Potential Impact\", #on users \n",
    "    'legend': { 0:'Low', 1:'High' },\n",
    "    'cpd': {\n",
    "        0: { 'Q': { 0: { 'D': { 0: 0.02, 1: 0.02 } }, # Low impact if: few users are affected and an employee try to use/not use the passwords\n",
    "                    1: { 'D': { 0: 0.03, 1: 0.98 } } } }, # Low impact if: a lot of users users are affected and an employee try to use/not use the passwords\n",
    "        1: { 'Q': { 0: { 'D': { 0: 0.98, 1: 0.08 } }, # High impact if: few users are affected and an employee try to use/not use the passwords\n",
    "                    1: { 'D': { 0: 0.03, 1: 0.02 } } } }, # High impact if: a lot of users are affected and an employee try to use/not use the passwords\n",
    "    }\n",
    "}\n",
    "    \n",
    "variables['R'] = {\n",
    "    'desc': \"Overall Risk\",\n",
    "    'legend': { 0:'Low', 1:'Medium', 2:'High', 3:'Critical' },\n",
    "    'cpd': {\n",
    "        # Low risk if: if the severity is low and the potential impact is low/high\n",
    "        0: { 'V': { 0: { 'I': { 0: 0.9, 1: 0.25 } }, \n",
    "        # Low risk if: if the severity is high and the potential impact is low/high\n",
    "                    1: { 'I': { 0: 0.4, 1: 0.01 } } } }, \n",
    "        # Medium risk if: if the severity is low and the potential impact is low/high\n",
    "        \n",
    "        1: { 'V': { 0: { 'I': { 0: 0.07, 1: 0.3 } },\n",
    "        # Medium risk if: if the severity is high and the potential impact is low/high    \n",
    "                    1: { 'I': { 0: 0.35, 1: 0.1 } } } },\n",
    "        # High risk if: if the severity is low and the potential impact is low/high\n",
    "        \n",
    "        2: { 'V': { 0: { 'I': { 0: 0.02, 1: 0.4 } }, \n",
    "        # High risk if: if the severity is high and the potential impact is low/high\n",
    "                    1: { 'I': { 0: 0.15, 1: 0.3 } } } }, \n",
    "        # Critical risk if: if the severity is low and the potential impact is low/high\n",
    "        \n",
    "        3: { 'V': { 0: { 'I': { 0: 0.01, 1: 0.05 } },\n",
    "        # Critical risk if: if the severity is high and the potential impact is low/high\n",
    "                    1: { 'I': { 0: 0.1, 1: 0.59 } } } }, \n",
    "    }\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Render the Graphical Representation of the Bayes Network\n",
    "\n",
    "Now that the structure and the variables are configured, we can render the visual representation of it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from graphviz_helper import render_graph\n",
    "from graphviz_helper import render_graph_probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "g = render_graph(structures, variables)\n",
    "\n",
    "g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = render_graph_probabilities(g, variables)\n",
    "\n",
    "g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from graphviz_helper import build_BayesianModel\n",
    "\n",
    "# Defining the model structure. We can define the network by just passing a list of edges.\n",
    "model = build_BayesianModel(structures, variables)\n",
    "\n",
    "model.check_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.get_cpds()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Printing the CPD for a node\n",
    "\n",
    "pgmpy allows us to display the probabilities associated with a variable. Here we are looking at the grade. Notice that pgmpy rotates the graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model.get_cpds('R'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can rotate the values to display them in a normal fashion by using the following. However, we lose the column headers and row descriptions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model.get_cpds('R').values.T)\n",
    "# print(model.get_cardinality('I'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting all the local independencies in the network.\n",
    "model.local_independencies(['V', 'R', 'I', 'D'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Active trail: For any two variables A and B in a network if any change in A influences the values of B then we say\n",
    "#               that there is an active trail between A and B.\n",
    "# In pgmpy active_trail_nodes gives a set of nodes which are affected by any change in the node passed in the argument.\n",
    "model.active_trail_nodes('R')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is the overall risk without evidence?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pgmpy.inference import VariableElimination\n",
    "infer = VariableElimination(model)\n",
    "print(infer.query(['R']) ['R'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Knowing what we know about the incident, what is the updated risk?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_variable = ['V' ,'I', 'R']\n",
    "evidence = {\n",
    "    'F': 1, # Abuse would have been detected\n",
    "    'S': 1, # Severity is high because of the type of data leaked\n",
    "    'Q': 1, # Quantity of data is high\n",
    "    'N': 1, # Employees did notice the presence of passwords in the logs\n",
    "    'A': 0, # Easy to exploit\n",
    "    'P': 0, # Slow to exploit\n",
    "    'B': 1, # Trying to get the data would have been detected\n",
    "\n",
    "}\n",
    "\n",
    "for q in query_variable:\n",
    "    if q not in evidence.keys():\n",
    "        print((infer.query([q], evidence=evidence) [q]))\n",
    "        print((infer.query([q], evidence=evidence) [q]).values)\n",
    "    else:\n",
    "        print(\"{} is observed as {}\".format(q,evidence[q]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predicting values from new data points\n",
    "\n",
    "Let's say that we don't care about the probability itself, we just want to know which is the most likely outcome for each variable.\n",
    "\n",
    "Predicting values from new data points is quite similar to computing the conditional probabilities. We need to query for the variable that we need to predict given all the other features. The only difference is that rather than getting the probabilitiy distribution we are interested in getting the most probable state of the variable.\n",
    "\n",
    "In pgmpy this is known as MAP query. \n",
    "\n",
    "Here's an example if we don't have evidence:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What is the most probable value for the variables `query_variable`, if we don't provide evidence?\n",
    "infer.map_query(query_variable)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Same query, but with evidences:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What is the most probable value for the variables `query_variable`, given evidences `evidence`?\n",
    "infer.map_query(query_variable, evidence=evidence)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
